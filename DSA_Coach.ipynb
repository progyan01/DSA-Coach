{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the dependencies"
      ],
      "metadata": {
        "id": "HTIqtpjNtsiN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NwtReiwBn13y",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q -U langchain langchain-community langchain-core langchain-text-splitters langchain-google-genai faiss-cpu langgraph langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the relevant libraries and setting up api key for gemini"
      ],
      "metadata": {
        "id": "9whisa9-t-te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.agents import create_agent\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.tools.retriever import create_retriever_tool\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "try:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "except:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3DVoSFS35q0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the source data for RAG from my github repo, then embedding and storing it accordingly"
      ],
      "metadata": {
        "id": "zd8t9vLFufYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_url = \"https://github.com/progyan01/DSA-Coach.git\"\n",
        "repo_name = \"DSA-Coach\"\n",
        "\n",
        "if not os.path.exists(repo_name):\n",
        "    print(f\"Cloning repository: {repo_url}\")\n",
        "    !git clone {repo_url}\n",
        "else:\n",
        "    print(\"Repository already exists. Pulling latest changes...\")\n",
        "    !cd {repo_name} && git pull\n",
        "\n",
        "print(\"Loading notes...\")\n",
        "loader = DirectoryLoader(f\"./{repo_name}/data\", glob=\"**/*.md\", loader_cls=TextLoader)     #all the notes are in the form of .md inside the /data folder\n",
        "docs = loader.load()\n",
        "print(f\"Loaded {len(docs)} documents.\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2500,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"```\", \"\\n$$\", \"\\n\\n\", \"\\n\", \" \"]               #as the notes is in .md format, using the relevant separators\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(f\"Created {len(splits)} chunks for the vector store.\")\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-m3\",\n",
        "    #not sure about the best model to use for this purpose, so chose the most recommended but size is 2gb due to more no. of parameters so needs a good internet\n",
        "    #using a local model to be run on colab instead of gemini embedding as I kept hitting rate limits on it\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    #as model is comparatively heavy, we use GPU for embedding\n",
        "    encode_kwargs={'normalize_embeddings': True},\n",
        "    #normalizes the vectors, improving performance\n",
        ")\n",
        "\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "#using FAISS vector database because its very fast\n",
        "\n",
        "print(\"Success!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cQj-GQCu5r6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the system prompt for the model and setting up the agent"
      ],
      "metadata": {
        "id": "KgUq57FOu0uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
        "#using low temperature to prevent hallucinations and give only the factual hints and information\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"search_dsa_notes\",\n",
        "    \"Searches the notes for algorithms, data structures, and code examples.\"\n",
        ")\n",
        "tools = [tool]\n",
        "#setting up the tool to be used by the agent\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are \"DSA-Coach\", an expert Technical Interviewer.\n",
        "Your goal is to help students solve problems by guiding them, NOT by giving the answer.\n",
        "\n",
        "### PHASE 1: ANALYZE THE INPUT\n",
        "The user input might contain:\n",
        "1. A Story/Problem Statement (e.g., \"Alice needs to visit cities...\").\n",
        "2. Code (optional).\n",
        "3. An Error Message (optional).\n",
        "\n",
        "**CRITICAL STEP (Story Translation):**\n",
        "If the user provides a \"Story Problem\" without naming an algorithm:\n",
        "1. You must DECONSTRUCT the story.\n",
        "2. Identify the underlying CS Concept (e.g., \"Cities are Nodes\" -> \"Graph\").\n",
        "3. Identify the Algorithm (e.g., \"Shortest path with weights\" -> \"Dijkstra\").\n",
        "4. ONLY THEN use the tool to search for that specific algorithm.\n",
        "\n",
        "### PHASE 2: DEBUGGING PROTOCOL\n",
        "- **Syntax Errors:** If the code has simple syntax errors (missing semicolon, indentation), point them out immediately. DO NOT use tools for this.\n",
        "- **Logic Errors (Silent Failures):** If the code runs but is wrong (or TLE), compare the user's logic to the Correct Algorithm found in the notes.\n",
        "- **Guardrail:** DO NOT write the full corrected code. Give a hint: \"You missed the base case for recursion.\"\n",
        "\"\"\"\n",
        "#giving the system prompt to fulfill all the project features\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "agent_executor = create_agent(\n",
        "    llm,\n",
        "    tools,\n",
        "    prompt=prompt\n",
        ")\n",
        "#using LangGraph for the agent because it can loop back think again and search for a different thing if it needs to; so the response is more refined and reliable\n",
        "\n",
        "print(\"Agent is ready to use!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8nYL78T4Dfv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the function for user prompt and model response"
      ],
      "metadata": {
        "id": "3T_GGj9RvAKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def ask_coach(code=None, error=None, question=\"\"):\n",
        "    print(\"(Type 'exit' to quit)\\n\")\n",
        "    while(True):\n",
        "      question = input(\"Enter Question: \")\n",
        "      if question.lower() == 'exit': break\n",
        "      code = input(\"Paste Code (press Enter if none): \")\n",
        "      error = input(\"Paste Error (press Enter if none): \")\n",
        "\n",
        "      user_context = f\"QUESTION: {question}\\n\\n\"\n",
        "      if code:\n",
        "          user_context += f\"STUDENT CODE:\\n{code}\\n\\n\"\n",
        "      if error:\n",
        "          user_context += f\"ERROR/ISSUE:\\n{error}\"\n",
        "      elif code:\n",
        "          user_context += \"ISSUE: Code runs but produces wrong output (Silent Failure).\"\n",
        "\n",
        "      #combining the different user prompts into a single prompt to give to the model\n",
        "\n",
        "      print(\"DSA-Coach is thinking...\")\n",
        "      try:\n",
        "          result = agent_executor.invoke({\n",
        "              \"messages\": [HumanMessage(content=user_context)]\n",
        "          })\n",
        "\n",
        "          answer = result[\"messages\"][-1].content\n",
        "          final_answer=answer[0][\"text\"]\n",
        "          #extracting the actual answer from the response given by the model\n",
        "\n",
        "          display(Markdown(f\"###Coach's Feedback\\n{final_answer}\"))\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error: {e}\")\n",
        "\n",
        "ask_coach()"
      ],
      "metadata": {
        "id": "ZLYhLP8PUSTh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}